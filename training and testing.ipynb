{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c7ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd1d173",
   "metadata": {},
   "source": [
    "# Data reading, normalization and padding\n",
    "1. read collected data\n",
    "2. normalization and padding them into the same size\n",
    "3. convert data to torch.tensor objects and send them to the device\n",
    "4. split the data into training set (70%) and testing set (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d3ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data collected\n",
    "f = open(\"data for training and testing/XYW_read data.pkl\", 'rb')\n",
    "X_sentence, Y_sentence, W_sentence =pkl.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93331331",
   "metadata": {
    "code_folding": [
     11,
     31
    ]
   },
   "outputs": [],
   "source": [
    "# padding and normalization\n",
    "Azure_vowels_list = [\"iy\", \"ih\", \"ey\", \"eh\", \"ae\", \"aa\", \"ao\", \"uh\", \"ow\", \"uw\", \"ah\", \"ay\", \"aw\", \"oy\", \"ax\", \"er\"]\n",
    "type_to_code = {Azure_vowels_list[i]:i+1 for i in range(len(Azure_vowels_list))}\n",
    "type_to_code[\"none\"] = 0\n",
    "\n",
    "# normalize and pad X_sentence, Y_sentence, W_sentence to a one with the length of 17\n",
    "factor_length = 12 # number of numerical features\n",
    "X_trans_P = []\n",
    "X_type_P = []\n",
    "X_type_Pcode= []\n",
    "Y_trans_P = []\n",
    "W_trans_P = []\n",
    "for sentence_ind in range(len(X_sentence)):\n",
    "    tp_sentence_feature = X_sentence[sentence_ind]\n",
    "    tp_sentence_stress = Y_sentence[sentence_ind]\n",
    "    tp_sentence_weight = W_sentence[sentence_ind]\n",
    "    # get the average syllable- and nuclei- level features for each sentence\n",
    "    tp_sentence_features = []\n",
    "    for word_ind in range(len(X_sentence[sentence_ind])):\n",
    "        # i is the index of syllable\n",
    "        tp_word_features = [X_sentence[sentence_ind][word_ind][i][:factor_length] for i in range(len(X_sentence[sentence_ind][word_ind]))]\n",
    "        tp_sentence_features.extend(tp_word_features)\n",
    "    tp_sentence_average = np.average(tp_sentence_features, axis=0)\n",
    "    # tp_sentence_average[tp_sentence_average==0] = 1 # make the zeros to be one\n",
    "    \n",
    "    for word_ind in range(len(X_sentence[sentence_ind])):\n",
    "        X_trans_P.append([np.array(X_sentence[sentence_ind][word_ind][i][:factor_length])-tp_sentence_average for i in range(len(X_sentence[sentence_ind][word_ind]))])\n",
    "        X_type_P.append([X_sentence[sentence_ind][word_ind][i][-1] for i in range(len(X_sentence[sentence_ind][word_ind]))])\n",
    "        Y_trans_P.append([Y_sentence[sentence_ind][word_ind][i] for i in range(len(Y_sentence[sentence_ind][word_ind]))])\n",
    "        W_trans_P.append([W_sentence[sentence_ind][word_ind][i] for i in range(len(W_sentence[sentence_ind][word_ind]))])\n",
    "\n",
    "seq_l = []\n",
    "for i in range(len(X_trans_P)):\n",
    "    seq_l.append(len(X_trans_P[i]))\n",
    "    tp_pad_type = [\"none\"]*(17-len(X_trans_P[i]))\n",
    "    tp_pad_x = [[0]*12]*(17-len(X_trans_P[i]))\n",
    "    tp_pad_y = [0]*(17-len(X_trans_P[i]))\n",
    "    X_trans_P[i] = X_trans_P[i]+tp_pad_x\n",
    "    Y_trans_P[i] = Y_trans_P[i]+tp_pad_y\n",
    "    W_trans_P[i] = W_trans_P[i]+tp_pad_y\n",
    "    \n",
    "    X_type_P[i] = X_type_P[i] + tp_pad_type\n",
    "    \n",
    "    X_type_Pcode.append([type_to_code[X_type_P[i][j]] for j in range(len(X_type_P[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01e6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to torch.tensor objects and send them to the device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_feature = torch.tensor(np.array(X_trans_P)).float().to(device)\n",
    "X_type = torch.tensor(X_type_Pcode).long().to(device)\n",
    "Y = torch.tensor(Y_trans_P).long().to(device)\n",
    "W = torch.tensor(W_trans_P).float().to(device)\n",
    "seq_T = torch.tensor(seq_l).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bfc2916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99430\n"
     ]
    }
   ],
   "source": [
    "train_set_prop = 0.7\n",
    "dataset = torch.utils.data.TensorDataset(X_feature, X_type, seq_T, Y, W)\n",
    "train_size = int(len(dataset)*train_set_prop)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6684e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5273a757",
   "metadata": {},
   "source": [
    "# Initializing and Training the model\n",
    "please specify the variables: \"use_all_features\", \"factor_size\", and \"n_head\", \"b_features_ph\", \"n_layer\"\n",
    "1. If the model is trained using all the numerical and categorical features, let use_all_features = True. If the model is trained using only numerical features, let use_all_features = False\n",
    "2. If numerical features are used only (i.e., use_all_features = False), specify the value of \"factor_size\" to indicate the frist n features to use. The first six features are measurements over syllable, while the next six features are measurements over nucleus. Models in the paper used factor_size=6 and factor_size=12\n",
    "3. \"n_head\", \"b_features_ph\", \"n_layer\" indicates the number of heads, features per head, and the number of layers of the transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4af033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model details\n",
    "use_all_features = False # set this to True if nucleus type is used, and False if only numerical features are used\n",
    "factor_size = 12 # the numebr of numerical features (the first n) to use if the model takes numerical features only\n",
    "\n",
    "# other parameters\n",
    "n_head = 5 # number of heads\n",
    "b_features_ph = 6 # number of transformed features per head\n",
    "n_layer = 3 # number of layers of the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40dabe51",
   "metadata": {
    "code_folding": [
     10,
     16
    ]
   },
   "outputs": [],
   "source": [
    "# specify training details\n",
    "max_iter = 100  # number of iterations (epochs)\n",
    "step_size = 2\n",
    "gamma = 1\n",
    "batch_size = 200\n",
    "\n",
    "lr = 0.001\n",
    "tp_lr = lr\n",
    "\n",
    "# initialize the model\n",
    "if use_all_features==False:\n",
    "    from self_attention_numerical import *\n",
    "    n_embed = n_head*b_features_ph\n",
    "    dropout = 0\n",
    "    config = Config(vocab_size=3, n_embed=n_embed, dropout=dropout, n_layer=n_layer, block_size=17, \n",
    "                    forward_expansion=3, n_head=n_head, fctor_size=factor_size, n_type=len(type_to_code))\n",
    "else:\n",
    "    from self_attention_all_features import *\n",
    "    n_embed = n_head*b_features_ph\n",
    "    dropout = 0\n",
    "    config = Config(vocab_size=3, n_embed=n_embed, dropout=dropout, n_layer=n_layer, block_size=17, \n",
    "                    forward_expansion=3, n_head=n_head, fctor_size=12, n_type=len(type_to_code))\n",
    "    \n",
    "model = TransModel(config).to(device)\n",
    "\n",
    "best_model = None\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "loss_recorder = []\n",
    "tp_ct = 0\n",
    "# tp_rec = []\n",
    "past_records = []\n",
    "tp_min_loss = 999999999\n",
    "lr_count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf575c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> start training\n"
     ]
    }
   ],
   "source": [
    "print(\"=> start training\")\n",
    "for epoch in range(max_iter):\n",
    "    accs = []\n",
    "    tp_rec = []\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    for batch_X, batch_X_type, batch_X_l, batch_Y, batch_W in train_loader:\n",
    "        tp_ct += 1\n",
    "        model.zero_grad()      \n",
    "        if use_all_features==False: # only numerical features are used\n",
    "            logits, loss, acc = model(batch_X[:, :, :factor_size], batch_X_l, batch_Y)\n",
    "        else: # numerical features and nucleus type are used\n",
    "            logits, loss, acc = model(batch_X, batch_X_type, batch_X_l, batch_Y, batch_W)\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accs.append(acc.cpu().detach())\n",
    "        loss_r = loss.cpu().detach()\n",
    "        loss_recorder.append(loss_r)\n",
    "        tp_rec.append(loss_r)\n",
    "    tp_epoch_loss = float(sum(tp_rec)/len(tp_rec))\n",
    "    print(\"current loss:\"+str(tp_epoch_loss)+\"  accuracy:\"+str(float(sum(accs)/len(accs))))\n",
    "    if tp_min_loss>tp_epoch_loss: # if the current loss is less than the min_loss record\n",
    "        tp_min_loss = float(tp_epoch_loss)\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()\n",
    "    if epoch%step_size==0:\n",
    "        tp_lr *= gamma\n",
    "    past_records.append(tp_epoch_loss)\n",
    "    # examine the losses and see if the model should be rolled-back\n",
    "    if epoch>6 and past_records[-1]*(1+1e-4)>sum(past_records[-4:-1])/3 and tp_lr<1e-5:\n",
    "        model = copy.deepcopy(best_model)\n",
    "        tp_lr *= 10\n",
    "        lr_count = 0\n",
    "        # print(tp_lr)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=tp_lr)\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    else:\n",
    "        lr_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b680770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fe5ba2e",
   "metadata": {},
   "source": [
    "# Look at the accuracy on the testing dataset\n",
    "When the nucleus features are used and weighted, there are weighted accuracy and unweighted accuracy.\n",
    "set batch_W=None when you want to print unweighted accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d94dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "loss_test=[]\n",
    "acc_test = []\n",
    "for batch_X, batch_X_type, batch_X_l, batch_Y, batch_W in test_loader:\n",
    "    with torch.no_grad():\n",
    "        if use_all_features==False: # only numerical features are used\n",
    "            logits, loss, acc = model(batch_X[:, :, :factor_size], batch_X_l, batch_Y)\n",
    "        else: # numerical features and nucleus type are used\n",
    "            # weighted accuracy\n",
    "            logits, loss, acc = model(batch_X, batch_X_type, batch_X_l, batch_Y, batch_W)\n",
    "            # unweighted accuracy\n",
    "#             logits, loss, acc = model(batch_X, batch_X_type, batch_X_l, batch_Y, None)\n",
    "            \n",
    "    loss_r = loss.cpu().detach()\n",
    "    acc_test.append(acc.cpu().detach())\n",
    "    loss_test.append(loss_r)\n",
    "    \n",
    "tp_epoch_loss = float(sum(loss_test)/len(loss_test))\n",
    "\n",
    "print(\"loss:\"+str(tp_epoch_loss)+\"  accuracy:\"+str(float(sum(acc_test)/len(acc_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094cf59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55767da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
